{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepBind_colab_finale.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "T_Goiz940CV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "41f5e88b-c037-41c2-fd7b-5114331fed08"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5ca88000 @  0x7f781968e2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "0.4.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aeIU5kFp0H1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "20ebe709-c144-4492-8338-baa088b20537"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O_pH6Jdz0VhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5acfcbaa-3e32-4a9e-e388-eae29a40ac4b"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Deepbind/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " best_hyperpamarameters_2.pth\n",
            " best_hyperpamarameters.pth\n",
            "'ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz'\n",
            "'ELK1_GM12878_ELK1_(1277-1)_Stanford_B.seq.gz'\n",
            " GABPA_GM12878_GABP_HudsonAlpha_AC.seq.gz\n",
            " GABPA_GM12878_GABP_HudsonAlpha_B.seq.gz\n",
            " MyModel_2.pth\n",
            " MyModel.pth\n",
            " USF1_HepG2_USF-1_HudsonAlpha_AC.seq.gz\n",
            " USF1_HepG2_USF-1_HudsonAlpha_B.seq.gz\n",
            "'ZBTB7A_HepG2_ZBTB7A_(SC-34508)_HudsonAlpha_AC.seq.gz'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hJ3u9Tweh2jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "203ad1ba-980a-420b-ca9f-2c4bff3327dc"
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "       process = psutil.Process(os.getpid())\n",
        "       print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "       print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 209.0 MB\n",
            "GPU RAM Free: 11430MB | Used: 11MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lHI_wW1cz8FN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math \n",
        "import random\n",
        "import gzip\n",
        "from scipy.stats import bernoulli\n",
        "import torch\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mOZziHBz8FT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nummotif=16 #number of motifs to discover\n",
        "bases='ACGT' #DNA bases\n",
        "basesRNA='ACGU'#RNA bases\n",
        "batch_size=64 #fixed batch size -> see notes to problem about it\n",
        "dictReverse={'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode\n",
        "reverse_mode=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJn4P52Fz8FX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seqtopad(sequence,motlen,kind='DNA'):\n",
        "    rows=len(sequence)+2*motlen-2\n",
        "    S=np.empty([rows,4])\n",
        "    base= bases if kind=='DNA' else basesRNA\n",
        "    for i in range(rows):\n",
        "        for j in range(4):\n",
        "            if i-motlen+1<len(sequence) and sequence[i-motlen+1]=='N' or i<motlen-1 or i>len(sequence)+motlen-2:\n",
        "                S[i,j]=np.float32(0.25)\n",
        "            elif sequence[i-motlen+1]==base[j]:\n",
        "                S[i,j]=np.float32(1)\n",
        "            else:\n",
        "                S[i,j]=np.float32(0)\n",
        "    return np.transpose(S)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lBcVVpC8z8Fb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dinucshuffle(sequence):\n",
        "    b=[sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
        "    random.shuffle(b)\n",
        "    d=''.join([str(x) for x in b])\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGWkcTgD7xrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def complement(seq):\n",
        "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}\n",
        "    complseq = [complement[base] for base in seq]\n",
        "    return complseq\n",
        "  \n",
        "def reverse_complement(seq):\n",
        "    seq = list(seq)\n",
        "    seq.reverse()\n",
        "    return ''.join(complement(seq))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7LnslLYz8Fj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Chip():\n",
        "    def __init__(self,filename,motiflen=24,reverse_complemet_mode=reverse_mode):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "            \n",
        "    def openFile(self):\n",
        "        train_dataset=[]\n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            if not self.reverse_complemet_mode:\n",
        "              for row in reader:\n",
        "                      train_dataset.append([seqtopad(row[2],self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(row[2]),self.motiflen),[0]])\n",
        "            else:\n",
        "              for row in reader:\n",
        "                      train_dataset.append([seqtopad(row[2],self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(reverse_complement(row[2]),self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(row[2]),self.motiflen),[0]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(reverse_complement(row[2])),self.motiflen),[0]])\n",
        "        #random.shuffle(train_dataset)\n",
        "        train_dataset_pad=train_dataset\n",
        "\n",
        "        size=int(len(train_dataset_pad)/3)\n",
        "        firstvalid=train_dataset_pad[:size]\n",
        "        secondvalid=train_dataset_pad[size:size+size]\n",
        "        thirdvalid=train_dataset_pad[size+size:]\n",
        "        firsttrain=secondvalid+thirdvalid\n",
        "        secondtrain=firstvalid+thirdvalid\n",
        "        thirdtrain=firstvalid+secondvalid\n",
        "        return firsttrain,firstvalid,secondtrain,secondvalid,thirdtrain,thirdvalid,train_dataset_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GwYhmKt6z8Fn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# chipseq=Chip('/content/drive/My Drive/Deepbind/ZBTB7A_HepG2_ZBTB7A_(SC-34508)_HudsonAlpha_AC.seq.gz')\n",
        "chipseq=Chip('/content/drive/My Drive/Deepbind/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Bn0w6urz8Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train1,valid1,train2,valid2,train3,valid3,alldataset=chipseq.openFile()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pX3Kgiaoz8Fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class chipseq_dataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,xy=None):\n",
        "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
        "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
        "        self.x_data = torch.from_numpy(self.x_data)\n",
        "        self.y_data = torch.from_numpy(self.y_data)\n",
        "        self.len=len(self.x_data)\n",
        "      \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yD_F4vtcz8F2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train1_dataset=chipseq_dataset(train1)\n",
        "train2_dataset=chipseq_dataset(train2)\n",
        "train3_dataset=chipseq_dataset(train3)\n",
        "valid1_dataset=chipseq_dataset(valid1)\n",
        "valid2_dataset=chipseq_dataset(valid2)\n",
        "valid3_dataset=chipseq_dataset(valid3)\n",
        "alldataset_dataset=chipseq_dataset(alldataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfxIcGZoz8F9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batchSize=64\n",
        "if reverse_mode:\n",
        "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  alldataset_loader=DataLoader(dataset=alldataset_dataset,batch_size=batchSize,shuffle=False)\n",
        "else:\n",
        "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batchSize,shuffle=True)\n",
        "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batchSize,shuffle=True)\n",
        "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batchSize,shuffle=True)\n",
        "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  alldataset_loader=DataLoader(dataset=alldataset_dataset,batch_size=batchSize,shuffle=False)\n",
        "\n",
        "train_dataloader=[train_loader1,train_loader2,train_loader3]\n",
        "valid_dataloader=[valid1_loader,valid2_loader,valid3_loader]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oHms3Olpz8GA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "import torch.nn.functional as F\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "def logsampler(a,b):\n",
        "        x=np.random.uniform(low=0,high=1)\n",
        "        y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
        "        return y\n",
        "    \n",
        "def sqrtsampler(a,b):\n",
        "        \n",
        "        x=np.random.uniform(low=0,high=1)\n",
        "        y=(b-a)*math.sqrt(x)+a\n",
        "        return y\n",
        "      \n",
        "# input of shape(batch_size,inp_chan,iW)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, nummotif,motiflen,poolType,neuType,mode,dropprob,learning_rate,momentum_rate,sigmaConv,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode=reverse_mode):\n",
        "      \n",
        "        super(ConvNet, self).__init__()\n",
        "        self.poolType=poolType\n",
        "        self.neuType=neuType\n",
        "        self.mode=mode\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "        self.dropprob=dropprob\n",
        "        self.learning_rate=learning_rate\n",
        "        self.momentum_rate=momentum_rate\n",
        "        self.sigmaConv=sigmaConv\n",
        "        self.sigmaNeu=sigmaNeu\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.beta3=beta3\n",
        "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
        "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
        "        self.wConv.requires_grad=True\n",
        "        \n",
        "        \n",
        "        self.wRect=torch.randn(nummotif).to(device)\n",
        "        torch.nn.init.normal_(self.wRect)\n",
        "        self.wRect=-self.wRect\n",
        "        self.wRect.requires_grad=True\n",
        "        \n",
        "        \n",
        "        if neuType=='nohidden':\n",
        "            \n",
        "            if poolType=='maxavg':\n",
        "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
        "            else:\n",
        "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "\n",
        "        else:\n",
        "            if poolType=='maxavg':\n",
        "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "            else:\n",
        "                \n",
        "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
        "            self.wNeu=torch.randn(32,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            self.wHiddenBias=torch.randn(32).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
        "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
        "            \n",
        "  \n",
        "            self.wHidden.requires_grad=True\n",
        "            self.wHiddenBias.requires_grad=True\n",
        "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
        "\n",
        "        self.wNeu.requires_grad=True\n",
        "        self.wNeuBias.requires_grad=True\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "   \n",
        "    def divide_two_tensors(self,x):\n",
        "        l=torch.unbind(x)\n",
        "\n",
        "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
        "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
        "        x1=torch.stack(list1,0)\n",
        "        x2=torch.stack(list2,0)\n",
        "        return x1,x2\n",
        "    def forward_pass(self,x,mask=None,use_mask=False):\n",
        "        \n",
        "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
        "        rect=conv.clamp(min=0)\n",
        "        maxPool, _ = torch.max(rect, dim=2)\n",
        "        if self.poolType=='maxavg':\n",
        "            avgPool= torch.mean(rect, dim=2)                          \n",
        "            pool=torch.cat((maxPool, avgPool), 1)\n",
        "        else:\n",
        "            pool=maxPool\n",
        "        if(self.neuType=='nohidden'):\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                  mask=bernoulli.rvs(self.dropprob, size=len(pool[0]))\n",
        "                  mask=torch.from_numpy(mask).float().to(device)\n",
        "                pooldrop=pool*mask\n",
        "                out=pooldrop @ self.wNeu\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(pool @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)       \n",
        "        else:\n",
        "            hid=pool @ self.wHidden\n",
        "            hid.add_(self.wHiddenBias)\n",
        "            hid=hid.clamp(min=0)\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                  mask=bernoulli.rvs(self.dropprob, size=len(hid[0]))\n",
        "                  mask=torch.from_numpy(mask).float().to(device)\n",
        "                hiddrop=hid*mask\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias) \n",
        "        return out,mask\n",
        "       \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not  self.reverse_complemet_mode:\n",
        "            out,_=self.forward_pass(x)\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            x1,x2=self.divide_two_tensors(x)\n",
        "            out1,mask=self.forward_pass(x1)\n",
        "            out2,_=self.forward_pass(x2,mask,True)\n",
        "            out=torch.max(out1, out2)\n",
        "     \n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_ACNZE2z8GE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        },
        "outputId": "592d2faf-ecc8-4646-fdf7-a86fc6ee4611"
      },
      "cell_type": "code",
      "source": [
        "best_AUC=0\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# device='cpu'\n",
        "learning_steps_list=[4000,8000,12000,16000,20000]\n",
        "for number in range(5):\n",
        "    \n",
        "    pool_List=['max','maxavg']        \n",
        "    random_pool=random.choice(pool_List)\n",
        "    \n",
        "    neuType_list=['hidden','nohidden']\n",
        "    random_neuType=random.choice(neuType_list)\n",
        "    dropoutList=[0.5,0.75,1.0] \n",
        "        \n",
        "    dropprob=random.choice(dropoutList)\n",
        "    \n",
        "    learning_rate=logsampler(0.0005,0.05)\n",
        " \n",
        "    momentum_rate=sqrtsampler(0.95,0.99)  \n",
        "\n",
        "    sigmaConv=logsampler(10**-7,10**-3)   \n",
        "    \n",
        "\n",
        "    sigmaNeu=logsampler(10**-5,10**-2) \n",
        "    beta1=logsampler(10**-15,10**-3)\n",
        "    beta2=logsampler(10**-10,10**-3)\n",
        "    beta3=logsampler(10**-10,10**-3)\n",
        "        \n",
        "        \n",
        "\n",
        "      \n",
        "    model_auc=[[],[],[]]\n",
        "    for kk in range(3):\n",
        "        model = ConvNet(16,24,random_pool,random_neuType,'training',dropprob,learning_rate,momentum_rate,sigmaConv,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode=reverse_mode).to(device)\n",
        "        if random_neuType=='nohidden':\n",
        "            optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "        else:\n",
        "            optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "        train_loader=train_dataloader[kk]\n",
        "        valid_loader=valid_dataloader[kk]\n",
        "        learning_steps=0\n",
        "        while learning_steps<=20000:\n",
        "            model.mode='training'\n",
        "            auc=[]\n",
        "            for i, (data, target) in enumerate(train_loader):\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                if model.reverse_complemet_mode:\n",
        "                  target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "                  for i in range(target_2.shape[0]):\n",
        "                    target_2[i]=target[2*i]\n",
        "                  target=target_2.to(device)\n",
        "                \n",
        "     \n",
        "                # Forward pass\n",
        "                output = model(data)\n",
        "                if model.neuType=='nohidden':\n",
        "                    loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "                else:\n",
        "                    loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta2*model.wHidden.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                learning_steps+=1\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "                if learning_steps% 4000==0:\n",
        "       \n",
        "                    with torch.no_grad():\n",
        "                        model.mode='test'\n",
        "                        auc=[]\n",
        "                        for i, (data, target) in enumerate(valid_loader):\n",
        "                            data = data.to(device)\n",
        "                            target = target.to(device)\n",
        "                            if model.reverse_complemet_mode:\n",
        "                              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "                              for i in range(target_2.shape[0]):\n",
        "                                target_2[i]=target[2*i]\n",
        "                              target=target_2.to(device)\n",
        "                            # Forward pass\n",
        "                            output = model(data)\n",
        "                            pred_sig=torch.sigmoid(output)\n",
        "                            pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "                            labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "                            \n",
        "                            auc.append(metrics.roc_auc_score(labels, pred))\n",
        "#                         print(np.mean(auc))\n",
        "                        model_auc[kk].append(np.mean(auc))\n",
        "                        print('AUC performance when training fold number ',kk+1, 'using ',learning_steps_list[len(model_auc[kk])-1],'learning steps = ',np.mean(auc))\n",
        "    \n",
        "    print('                   ##########################################               ')\n",
        "    for n in range(5):\n",
        "        AUC=(model_auc[0][n]+model_auc[1][n]+model_auc[2][n])/3\n",
        "        #print(AUC)\n",
        "        if AUC>best_AUC:\n",
        "            best_AUC=AUC\n",
        "            best_learning_steps=learning_steps_list[n]\n",
        "            best_LearningRate=model.learning_rate\n",
        "            best_LearningMomentum=model.momentum_rate\n",
        "            best_neuType=model.neuType\n",
        "            best_poolType=model.poolType\n",
        "            best_sigmaConv=model.sigmaConv\n",
        "            best_dropprob=model.dropprob\n",
        "            best_sigmaNeu=model.sigmaNeu\n",
        "            best_beta1=model.beta1\n",
        "            best_beta2=model.beta2\n",
        "            best_beta3=model.beta3\n",
        "            \n",
        "print('best_poolType=',best_poolType)\n",
        "print('best_neuType=',best_neuType)\n",
        "print('best_AUC=',best_AUC)            \n",
        "print('best_learning_steps=',best_learning_steps)      \n",
        "print('best_LearningRate=',best_LearningRate)\n",
        "print('best_LearningMomentum=',best_LearningMomentum)\n",
        "print('best_sigmaConv=',best_sigmaConv)\n",
        "print('best_dropprob=',best_dropprob)\n",
        "print('best_sigmaNeu=',best_sigmaNeu)\n",
        "print('best_beta1=',best_beta1)\n",
        "print('best_beta2=',best_beta2)\n",
        "print('best_beta3=',best_beta3)\n",
        "\n",
        "best_hyperparameters = {'best_poolType': best_poolType,'best_neuType':best_neuType,'best_learning_steps':best_learning_steps,'best_LearningRate':best_LearningRate,\n",
        "                        'best_LearningMomentum':best_LearningMomentum,'best_sigmaConv':best_sigmaConv,'best_dropprob':best_dropprob,\n",
        "                        'best_sigmaNeu':best_sigmaNeu,'best_beta1':best_beta1, 'best_beta2':best_beta2,'best_beta3':best_beta3}\n",
        "torch.save(best_hyperparameters, '/content/drive/My Drive/Deepbind/best_hyperpamarameters.pth')\n",
        "            \n",
        " "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.7573384510206571\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.7527348832953806\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.7504746737590816\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.7475097656249999\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.742764288183149\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.7766792723548407\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.7593681422667891\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.7493275434713288\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.741010302152388\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.7338485901682318\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.7808474771635155\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.7802339679255098\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.7775217629296323\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.7680982513937811\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.760048561489018\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.7729437439851713\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.7643625425273802\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.7571588639089948\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.7594257977695998\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.7550910653532314\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.8114024529825778\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.802618866489575\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.7929359874933102\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.7899652572219581\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.7871190746596379\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.782153998952561\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.7637586193312796\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.7619983672325455\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.7622254460029053\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.7629272003581204\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.7693100144355888\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.7648408802489969\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.7622571195239645\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.7583899659103231\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.7659962780070755\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.7513979859518486\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.7908381519350344\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.8011499513457475\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.7959432630473113\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.7936228129932901\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.7736301045125413\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.7806688124326947\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.7590305904429251\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.7600271999374622\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.756995530199406\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.7388451084194047\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.7219138316491813\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.7127319203568911\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.7105834087304815\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.7128419451278193\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.7486290123336806\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.7385828277856297\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.7311833134347866\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.719439736428871\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.7134900742818298\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.7252962310423351\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.7130880659869062\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.7029095057633685\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.6951308720385659\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.6955504841162952\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.7592454270358924\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.7694105510260789\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.759694855457737\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.755300324207737\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.7519517377771904\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.7226607474576486\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.8134628130602677\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.7825945783239655\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.7720399899229322\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.7722721072256509\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.7327843202793731\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.7997097326646737\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.7830572858534034\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.7764660860063829\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.7728839071754481\n",
            "                   ##########################################               \n",
            "best_poolType= max\n",
            "best_neuType= hidden\n",
            "best_AUC= 0.7941943655836735\n",
            "best_learning_steps= 8000\n",
            "best_LearningRate= 0.0008773389612059864\n",
            "best_LearningMomentum= 0.968921636146553\n",
            "best_sigmaConv= 0.0001568185383200897\n",
            "best_dropprob= 0.75\n",
            "best_sigmaNeu= 0.000975369251624566\n",
            "best_beta1= 5.337865608613222e-14\n",
            "best_beta2= 8.905486160913463e-05\n",
            "best_beta3= 1.307760197744172e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rwZUA6cmRtlN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input of shape(batch_size,inp_chan,iW)\n",
        "class ConvNet_test(nn.Module):\n",
        "    def __init__(self,nummotif,motiflen,poolType,neuType,mode,learning_steps,learning_rate,learning_Momentum,sigmaConv,dropprob,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode):\n",
        "        super(ConvNet_test, self).__init__()\n",
        "        self.poolType=poolType\n",
        "        self.neuType=neuType\n",
        "        self.mode=mode\n",
        "        self.learning_rate=learning_rate\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "        self.momentum_rate=learning_Momentum\n",
        "        self.sigmaConv=sigmaConv\n",
        "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
        "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
        "        self.wConv.requires_grad=True\n",
        "        self.wRect=torch.randn(nummotif).to(device)\n",
        "        torch.nn.init.normal_(self.wRect)\n",
        "        self.wRect=-self.wRect\n",
        "        self.wRect.requires_grad=True\n",
        "        self.dropprob=dropprob\n",
        "        self.sigmaNeu=sigmaNeu\n",
        "        self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "        self.wHiddenBias=torch.randn(32).to(device)\n",
        "        if neuType=='nohidden':\n",
        "            \n",
        "            if poolType=='maxavg':\n",
        "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
        "            else:\n",
        "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "\n",
        "        else:\n",
        "            if poolType=='maxavg':\n",
        "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "            else:\n",
        "                \n",
        "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
        "            self.wNeu=torch.randn(32,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            self.wHiddenBias=torch.randn(32).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
        "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
        "            \n",
        "  \n",
        "            self.wHidden.requires_grad=True\n",
        "            self.wHiddenBias.requires_grad=True\n",
        "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
        "\n",
        "        self.wNeu.requires_grad=True\n",
        "        self.wNeuBias.requires_grad=True\n",
        "        \n",
        "\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.beta3=beta3\n",
        "        \n",
        "\n",
        "    \n",
        "    def divide_two_tensors(self,x):\n",
        "        l=torch.unbind(x)\n",
        "\n",
        "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
        "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
        "        x1=torch.stack(list1,0)\n",
        "        x2=torch.stack(list2,0)\n",
        "        return x1,x2\n",
        "    def forward_pass(self,x,mask=None,use_mask=False):\n",
        "        \n",
        "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
        "        rect=conv.clamp(min=0)\n",
        "        maxPool, _ = torch.max(rect, dim=2)\n",
        "        if self.poolType=='maxavg':\n",
        "            avgPool= torch.mean(rect, dim=2)                          \n",
        "            pool=torch.cat((maxPool, avgPool), 1)\n",
        "        else:\n",
        "            pool=maxPool\n",
        "        if(self.neuType=='nohidden'):\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                    mask=bernoulli.rvs(self.dropprob, size=len(pool[0]))\n",
        "                    mask=torch.from_numpy(mask).float().to(device)\n",
        "                pooldrop=pool*mask\n",
        "                out=pooldrop @ self.wNeu\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(pool @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)       \n",
        "        else:\n",
        "            hid=pool @ self.wHidden\n",
        "            hid.add_(self.wHiddenBias)\n",
        "            hid=hid.clamp(min=0)\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                    mask=bernoulli.rvs(self.dropprob, size=len(hid[0]))\n",
        "                    mask=torch.from_numpy(mask).float().to(device)\n",
        "                hiddrop=hid*mask\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias) \n",
        "        return out,mask\n",
        "       \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not  self.reverse_complemet_mode:\n",
        "            out,_=self.forward_pass(x)\n",
        "        else:\n",
        "            \n",
        "            x1,x2=self.divide_two_tensors(x)\n",
        "            out1,mask=self.forward_pass(x1)\n",
        "            out2,_=self.forward_pass(x2,mask,True)\n",
        "            out=torch.max(out1, out2)\n",
        "        \n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkrEJTXuz8GW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cdc778fd-1557-47cd-bccb-7ffcc5ea8fea"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "learning_steps_list=[4000,8000,12000,16000,20000]\n",
        "best_AUC=0\n",
        "\n",
        "best_hyperparameters = torch.load('/content/drive/My Drive/Deepbind/best_hyperpamarameters.pth')\n",
        "\n",
        "best_poolType=best_hyperparameters['best_poolType']\n",
        "best_neuType=best_hyperparameters['best_neuType']\n",
        "best_learning_steps=best_hyperparameters['best_learning_steps']\n",
        "best_LearningRate=best_hyperparameters['best_LearningRate']\n",
        "best_dropprob=best_hyperparameters['best_dropprob']\n",
        "best_LearningMomentum=best_hyperparameters['best_LearningMomentum']\n",
        "best_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
        "best_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
        "best_beta1=best_hyperparameters['best_beta1']\n",
        "best_beta2=best_hyperparameters['best_beta2']\n",
        "best_beta3=best_hyperparameters['best_beta3']\n",
        "\n",
        "\n",
        "for number_models in range(6):\n",
        "\n",
        "  model = ConvNet_test(16,24,best_poolType,best_neuType,'training',best_learning_steps,best_LearningRate,best_LearningMomentum,best_sigmaConv,best_dropprob,best_sigmaNeu,best_beta1,best_beta2,best_beta3,reverse_complemet_mode=False).to(device)\n",
        "\n",
        "  if model.neuType=='nohidden':\n",
        "      optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "  else:\n",
        "      optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_loader=alldataset_loader\n",
        "  valid_loader=alldataset_loader\n",
        "  learning_steps=0\n",
        "  while learning_steps<=best_learning_steps:\n",
        "  \n",
        "      for i, (data, target) in enumerate(train_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "            # Forward pass\n",
        "          output = model(data)\n",
        "          \n",
        "          if model.neuType=='nohidden':\n",
        "              loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "          else:\n",
        "              loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta2*model.wHidden.norm()+model.beta3*model.wNeu.norm()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          learning_steps+=1\n",
        "          \n",
        "  with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "      for i, (data, target) in enumerate(valid_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "\n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #             \n",
        "      AUC_training=np.mean(auc)\n",
        "      print('AUC for model ',number_models,' = ',AUC_training)\n",
        "      if AUC_training>best_AUC:\n",
        "          state = {'conv': model.wConv,'rect':model.wRect,'wHidden':model.wHidden,'wHiddenBias':model.wHiddenBias,'wNeu':model.wNeu,'wNeuBias':model.wNeuBias }\n",
        "          torch.save(state, '/content/drive/My Drive/Deepbind/MyModel_2.pth')\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        " "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "AUC for model  0  =  0.8911117144381336\n",
            "AUC for model  1  =  0.866348566254332\n",
            "AUC for model  2  =  0.8811529328712616\n",
            "AUC for model  3  =  0.889619608161661\n",
            "AUC for model  4  =  0.8890761124614939\n",
            "AUC for model  5  =  0.9180261579867796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JetF0ajS4faT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a05ac166-fbef-4725-8a8d-221ab8c7e088"
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('/content/drive/My Drive/Deepbind/MyModel_2.pth')\n",
        "model = ConvNet_test(16,24,best_poolType,best_neuType,'test',best_learning_steps,best_LearningRate,best_LearningMomentum,best_sigmaConv,best_dropprob,best_sigmaNeu,best_beta1,best_beta2,best_beta3,reverse_complemet_mode=reverse_mode).to(device)\n",
        "model.wConv=checkpoint['conv']\n",
        "model.wRect=checkpoint['rect']\n",
        "model.wHidden=checkpoint['wHidden']\n",
        "model.wHiddenBias=checkpoint['wHiddenBias']\n",
        "model.wNeu=checkpoint['wNeu']\n",
        "model.wNeuBias=checkpoint['wNeuBias']\n",
        "\n",
        "with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "     \n",
        "      for i, (data, target) in enumerate(valid_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "\n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #                         \n",
        "      AUC_training=np.mean(auc)\n",
        "      print(AUC_training)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9180261579867796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AObJY0ZjjNLW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Chip_test():\n",
        "    def __init__(self,filename,motiflen=24,reverse_complemet_mode=reverse_mode):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "            \n",
        "    def openFile(self):\n",
        "        test_dataset=[]\n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            if not self.reverse_complemet_mode:\n",
        "              for row in reader:\n",
        "                      test_dataset.append([seqtopad(row[2],self.motiflen),[int(row[3])]])\n",
        "            else:\n",
        "              for row in reader:\n",
        "                      test_dataset.append([seqtopad(row[2],self.motiflen),[int(row[3])]])\n",
        "                      test_dataset.append([seqtopad(reverse_complement(row[2]),self.motiflen),[int(row[3])]])\n",
        "            \n",
        "        return test_dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZObdD8Yaj8w1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chipseq_test=Chip_test('/content/drive/My Drive/Deepbind/ELK1_GM12878_ELK1_(1277-1)_Stanford_B.seq.gz')\n",
        "test_data=chipseq_test.openFile()\n",
        "test_dataset=chipseq_dataset(test_data)\n",
        "batchSize=test_dataset.__len__()\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batchSize,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4oqk1HnfQQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d57c9cc3-de9f-4c97-f07d-31c2e5d9ea50"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "     \n",
        "      for i, (data, target) in enumerate(test_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "          \n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #                         \n",
        "      AUC_training=np.mean(auc)\n",
        "      print('AUC on test data = ',AUC_training)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC on test data =  0.899808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AtHbPeAohR6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}